Much of machine learning is concerned with the analysis of given data. By contrast, the field of optimal experiment design (OED) is concerned with the creation of new data by experimentation or query. In many contexts, careful design of the experiment leads to more efficient learning. The gain in efficiency can be dramatic \cite{myung2013}.

Idealised active learning can be represented as follows
\begin{center}
\begin{tikzpicture}
	[every node/.style={inner sep=10,outer sep=0,rounded corners}]
	\node (design) [draw, align=center] at (0, 0) {Design next \\ experiment};
	\node (perform) [draw, align=center] at (2, -3.46) {Perform \\ experiment};
	\node [draw] (analyse) at (-2, -3.46) {Analyse data};
	\draw [->] (design) -- (perform);
	\draw [->] (perform) -- (analyse);
	\draw [->] (analyse) -- (design);
\end{tikzpicture}
\end{center}
It is only within the context of a proposed data analysis that the optimality or suboptimality of an experiment design can be assessed.

\section{Foundations}
\subsection{Problem specification}
We assume the data analysis model for the experiment takes the form given by the graphical model
\begin{center}
\begin{tikzpicture}
	\tikzstyle{every node}=[draw,shape=circle]
	\node[draw,fill=lightgray] (y) at (0, 0) {$y$};
	\node (t) at (2, 1) {$\theta$};
	\node[draw,shape=rectangle] (d) at (-2, 1) {$d$};
	\draw [->] (d) -- (y);
	\draw [->] (t) -- (y);
\end{tikzpicture}
\end{center}
in which $d$ represents the (non-random) design of the experiment, $\theta$ represents a latent variable and $y$ represents the observed outcome of the experiment. The joint density is
\begin{equation}
	p(y, \theta | d) = p(\theta)p(y|\theta,d)
\end{equation}
We can generalize to the sequential design setting by 
standard Bayesian updating, namely at experiment iteration $t$, we replace $p(\theta)$ with 
$p(\theta | d_{1:t-1}, y_{1:t-1})$, where $d_{1:t-1}$ and  $y_{1:t-1}$ are the designs and outcomes at previous steps of the experiment.
The likelihood $p(y_t | \theta, d_t)$ is assumed unchanged. That is, experimentation does not change the behaviour of the underlying system from time to time, conditional upon $\theta$.


\subsection{One-step design}
We begin with the case of designing one experiment, after which we will collect and analyse our data. In selecting an optimal experiment design, we must first say with respect to what utility a design is considered optimal. We begin by very briefly discussing a range of choices for utility. We then focus on a natural Bayesian choice of utility, the expected information gain.

Suppose $\mathcal{D}$ is the space of admissible designs. We seek a utility $U: \mathcal{D} \to \R$. The optimal design would then be
\begin{equation}
	d^* = \argmax_{d \in \mathcal{D}} \ U(d).
\end{equation}

Suppose initially that we can assign a real-valued utility $U(y, \theta, d)$ to the event of observing $y$ under design $d$ when the true latent variable was $\theta$. We can average over $y$ to obtain
\begin{equation}
	U(\theta, d) = \int p(y | \theta, d)\ U(y, \theta, d)\ dy
\end{equation}
We can deal with $\theta$ in various ways
\begin{itemize}
	\item The Bayesian approach \cite{chaloner1995} places a prior $p(\theta)$ on $\theta$ and takes \\ $U(d) = \int p(\theta)\ U(\theta, d)\ d\theta$
	\item The minimax approach \cite{fedorov1972} takes $U(d) = \inf_\theta U(\theta, d)$
	\item The local approach \cite{pronzato2010} begins with an estimate $\hat{\theta}$ and sets $U(d) = U(\hat{\theta}, d)$
\end{itemize}
Here, we primarily focus on the Bayesian approach. This is not an arbitrary decision, it can be motivated from decision theoretic considerations \cite{lindley1972}. See \cite{chaloner1995} for further discussion of Bayesian experimental design.

Alternatively, we could take the (matrix-valued) `utility'
\begin{equation}
	U(y, \theta, d) = \left( \frac{\partial}{\partial \theta} \log p(y | \theta, d) \right)^2
\end{equation}
which leads to the the Fisher Information Matrix, used in many experiment design criteria \cite{pronzato2010}, defined as
\begin{equation}
	\mathcal{I}(\theta, d) = \int \left( \frac{\partial}{\partial \theta} \log p(y | \theta, d) \right)^2 p(y | \theta, d)\ dy
\end{equation}
We can obtain a scalar utility from $\mathcal{I}(\theta, d)$ by choosing from the `alphabetical' criteria \cite{box1982} which are defined as
\begin{itemize}
	\item D-optimality $U(d, \theta) = \text{det } \mathcal{I}(\theta, d)$
	\item A-optimality $U(d, \theta) = \text{tr } \mathcal{I}(\theta, d)$
	\item E-optimality $U(d, \theta) = \max_i \lambda_i$ where $\lambda_i$ are the eigenvalues of $\mathcal{I}(\theta, d)$
\end{itemize}

Work dating back to \cite{lindley1956} instead uses an information-theoretic utility\footnote{It can also be shown \cite{chaloner1995} that this utility leads to a (modified form) of D-optimality for linear models.}
\begin{equation}
	U(y, \theta, d) = \log \frac{p(\theta | y, d)}{p(\theta)} = \log \frac{p(y | \theta, d)}{p(y|d)}
\end{equation}
and Lindley established that this is the only form that satisfies certain intuitive properties of an informative experiment. For this reason, we will focus on this utility. See \cite{chaloner1995, ryan2015} for a fuller discussion of utility functions used in experiment design.

With the information-theoretic utility and Bayesian averaging, we arrive at the following form for $U(d)$, called the \textbf{expected information gain (EIG)}
\begin{equation}
	U(d) = \eig(d) = \iint p(y, \theta | d) \log\frac{p(\theta | y, d)}{p(\theta)} \ dy\ d\theta
\end{equation}
The EIG can be interpreted in a number of ways
\begin{enumerate}
\item As the expectation of information gain. If we define
\begin{equation}
	\text{IG}(y, d) = \text{KL}(\ p(\theta | y, d)\ ||\ p(\theta)\ ) 
\end{equation}
then $\eig(d) = \expect_{y\sim p(y|d)}[\text{IG}(y, d)] $.
\item From APE. Define the \textbf{average posterior entropy} (APE) as 
\begin{align}
	\ape(d) &= \iint p(y, \theta | d) \log p(\theta | y, d) \ dy \ d\theta \\
	&= -\int p(y|d) \entropy{p(\theta | y, d)} dy
\end{align}
where $H$ is the differential entropy.
Then
\begin{equation}
	\label{eq:apeeig}
	\eig(d) = \entropy{p(\theta)} - \ape(d)
\end{equation}
and the prior entropy is a constant w.r.t. $d$. Thus EIG maximisation corresponds to APE
minimisation.
\item Mutual information. Recall the mutual information is defined as
\begin{equation}
	\text{MI}(x, y) = \kl{p(x, y)}{p(x)p(y)}
\end{equation}
then we have
\begin{align}
	\text{MI}(y, \theta | d) &= \kl{p(y, \theta | d)}{p(y|d)p(\theta)} \\
	&= \iint p(y, \theta | d) \log \frac{p(y, \theta | d)}{p(\theta)p(y|d)} dy \, d\theta \\
	&= \eig(d).
\end{align}
\item Epistemic uncertainty.
The total entropy or uncertainty in response $y$ is
\begin{equation}
	H_{tot} = \entropy{p(y|d)}
\end{equation}
the aleatoric uncertainty under parameter $\theta$ is
\begin{equation}
	H_{alea}(\theta) = \entropy{p(y|\theta, d)}
\end{equation}
Under prior $p(\theta)$, the expected aleatoric uncertainty is
\begin{equation}
	H_{alea} = \expect_{\theta \sim p(\theta)}\left[ \entropy{p(y|\theta, d)} \right]
\end{equation}
The epistemic uncertainty, under $p(\theta)$, is
\begin{align}
	H_{epist} =& H_{tot} - H_{alea}\\
	=&\entropy{p(y|d)} - \expect_{\theta \sim p(\theta)}\left[ \entropy{p(y|\theta, d)} \right] \\
	=&-\int p(y|d)\log p(y|d)dy + \iint p(y, \theta | d)\log p(y|\theta, d) dy \, d\theta\\
	=& \eig(d)
\end{align}

\end{enumerate}
We shall see that the connection to mutual information in particular is handy for estimating EIG, because modern techniques for estimating MI have been developed in the recent past.


\subsection{Multi-step design}
We turn to the case of multi-step, or adaptive, experiment design. In this context, we are able to base the next experiment on previous experimental designs and outcomes. Designing a sequence of multiple experiments, with a view to maximise expected utility can be viewed as a Partially Observable Markov Decision Process (POMDP) \cite{marchant2014}, and falls within the scope of reinforcement learning \cite{pang2018}. The problem is also referred to as backward induction or stochastic dynamic programming. The formal reframing of OED as a POMDP is laid out below.

\subsubsection{Setup}
Suppose we have a determinic, finite time horizon $t=1, ..., T$. We specify as Partially Observable Markov Decision Process (POMDP) as follows.
\begin{itemize}
\item States $s_t = (\theta, h_t)$, where $h_t = d_{1:t}, y_{1:t}$ the history of designs $d$ and outcomes $y$ up to the current time. Here $y_t \sim p(y | \theta, d)$ is the outcome of performing the experiment using design $d_t$. The practical state $s'_t$ consists of the sufficient statistics for $\theta$ obtained from $h_t$. These can be used to compute the belief states $b_t$, encoding the full posterior for $\theta$ given that history $h_t$.
\item Actions $a_t = d_{t+1}$. Transitions correspond to running the experiment and producing the outcome $y_{t+1}$.
\item Observations $o_t = h_t$. Thus the only unobserved part of the state $(\theta, h_t)$ is the latent $\theta$.
\item Rewards $r_t = r(t, \theta, h_t)$. We take $r$ to be a non-random function. Note that in many OED settings, we take $r_t = 0$ for $t<T$. Intuitively, this means we only care about our final understanding of or action upon the system, not the path taken to it. This is the choice made by \cite{gonzalez2016} among others.
\end{itemize}
Under this set-up, the \textit{optimal experiment design policy} is a map $\pi$ from histories $h_t$ to actions $a_t$ which maximises the total reward
\begin{equation}
	R_T = \expect \left[ \sum_{t=1}^T \gamma^t r_t \bigg{\rvert} \pi \right]
\end{equation}
where $\gamma \in [0,1]$ is the discount factor, often set to 1 in finite horizons.


\subsubsection{Connection to EIG}
\paragraph{Horizon 1}
Suppose $T=1$. Choose the following reward function
\begin{equation}
r(1, \theta, h) = \log \frac{p(\theta | y, d)}{p(\theta)} = \log \frac{p(y | \theta, d)}{p(y|d)}
\end{equation}
The $Q$-function of action $d_1$ is the expected reward
\begin{equation}
	Q(s_0, d_1) = E_{y \sim p(y|\theta, d)}[r(t, \theta, d_1, y)]
\end{equation}
Since we have no observation of $\theta$, the belief $Q$-function of the belief state $p(\theta)$ and action $d_1$ is
\begin{equation}
	Q(p(\theta), d_1) = E_{\theta \sim p(\theta)}\{E_{y \sim p(y|\theta, d)}[r(t, \theta, d_1, y)]\}
\end{equation}
which reduces to the familiar expression
\begin{equation}
	Q(p(\theta), d_1) = \iint p(y, \theta | d) \log \frac{p(\theta | y, d)}{p(\theta)} \, d\theta \, dy
\end{equation}

\paragraph{Horizon $T$}
This formalism provides a convenient way to avoid the greedy approach to sequential design that is compatible with the information-theoretic objective of \cite{lindley1956}.

Suppose the belief at time $t$ is $b_t(\theta)$. This can be computed from the sufficient stats $s'_t$. We take the reward to be $0$ at $t<T$ and
\begin{equation}
	r(T, \theta, h_T) = r(T, \theta, b_T(\theta)) = \log \frac{b_T(\theta) }{p(\theta)}
\end{equation}
we have updated $b$ according to Bayes Theorem so
\begin{equation}
	b_T(\theta) = p(\theta | y_{1:T}, d_{1:T})
\end{equation}
This reward structure represents the total information gained about $\theta$ from all experiments.

In fact, we can rewrite this reward to take non-zero values at earlier times, setting
\begin{equation}
	\label{eq:rlmutli}
	r(t, \theta, h_t) = r(t, \theta, b_t(\theta)) = \log\frac{b_t(\theta)}{b_{t-1}(\theta)}
\end{equation}
and this is equivalent to the previous formulation. To see this, consider the belief $Q$-function
\begin{align}
	Q(b_t(\theta), d_{t+1}) =& \int b_t(\theta)  \int p(y_{t+1} | \theta, d_{t+1}) \log\frac{b_{t+1}(\theta)}{b_t(\theta)} \\ &+ \int p(y_{t+2} | \theta, d_{t+1}, y_{t+1}) \log \frac{b_{t+2}(\theta)}{b_{t+1}(\theta)} + \hdots \, dy_{t+2} \, dy_{t+1} \, d\theta \\
	=& \int b_t(\theta) \int p(y_{t+1:t+2}|\theta, d_{t+1}) \log \frac{b_{t+2}(\theta)}{b_t(\theta)} + \hdots \, dy_{t+1:t+2} \, d\theta \\
	=& \hdots \\
	=& \int b_t(\theta) \int p(y_{t+1:T}|\theta,d_{t+1}) \log \frac{b_T(\theta)}{b_t(\theta)} \, dy_{t+1:T} \, d\theta
\end{align}
where $p(y_{t+1:T}|\theta, d_{t+1})$ assumes an optimal strategy after step $t+1$, under reward \eqref{eq:rlmutli}. We can now see by induction that the optimal strategy and $Q$-functions are the same for either choice of reward structure.

\subsubsection{The greedy approach}
In reinforcement learning, greediness refers to maximising the one-step-ahead reward, namely
\begin{equation}
	a_t = \argmax_{a_t \in \mathcal{A}} \left(\expect[r_{t+1} | a_t] \right)
\end{equation}
which, with the reward of \eqref{eq:rlmutli}, corresponds to one-step EIG maximisation at each step. We primarily focus on this form of multi-step optimisation because it removes all aspects of future planning beyond a single step from an already difficult problem.

\subsubsection{Non-greedy approaches}
Some have considered non-greedy strategies \cite{gonzalez2016} \cite{pang2018}. See \cite[sec 6.1]{ryanreview} for a summary of `backwards induction' approaches.


\subsubsection{Optional stopping}

We finally mention another possible complication. Rather than a fixed and finite time horizon $T$, we may we allowed to continue experimentation indefinitely, choosing when to stop. One natural choice to stopping criterion is to terminate when the posterior entropy reaches a threshold value.

Optional stopping in Bayesian data analysis is a hotly debated topic \\ \cite{rouder2014, de2017}, but can be rigorously justified using martingale theory with certain stopping rules \cite{shafer2011} in the context of hypothesis testing. Of particular interest are stopping criteria based on non-negative supermartingales. We note in passing that the posterior entropy is a supermartingale (see \eqref{eq:apeeig}) and is non-negative if $\theta$ is discrete.


\subsection{Theoretical considerations}
The proposed experimentation strategy in which we design future experiments on the basis of previous observations may, at first sight, cause some consternation to the theoretical statistician. The first question we seek to answer is `in what sense is the posterior obtained from multi-step OED the same as that obtained by a pre-ordained experimentation strategy?' The second line of questioning concerns the asymptotics of multi-step OED. `Is multi-step OED statistically consistent and does it provide a faster convergence rate than other methods?'

To the first question, the answer is simply that the posterior is the same as if the experimentation strategy had been pre-ordained. Indeed, let us regard the designs as random variables and consider a 2-step experiment with the following graphical model
\begin{center}
\begin{tikzpicture}
	\tikzstyle{every node}=[draw,shape=circle]
	\node (t) at (0, -1) {$\theta$};
	\node[draw,fill=lightgray] (y1) at (-2, -2) {$y_1$};
	\node[draw,fill=lightgray] (y2) at (2, -2) {$y_2$};
	\node[draw,fill=lightgray] (d1) at (-2, -4) {$d_1$};
	\node[draw,fill=lightgray] (d2) at (2, -4) {$d_2$};
	\draw [->] (d1) -- (y1);
	\draw [->] (t) -- (y1);
	\draw [->] (d2) -- (y2);
	\draw [->] (t) -- (y2);
	\draw [->] (d1) -- (d2);
	\draw [->] (y1) -- (d2);
\end{tikzpicture}
\end{center}
and the following conditional density for $\theta$
\begin{align}
	p(\theta | y_1, d_1, y_2, d_2) =& \frac{p(\theta, y_1, d_1, y_2, d_2)}{p(y_1, d_1, y_2, d_2)} \\
	=& \frac{p(\theta)p(d_1)p(y_1|\theta, d_1)p(d_2|y_1,d_1)p(y_2|\theta,d_2)}{\int p(\theta)p(d_1)p(y_1|\theta, d_1)p(d_2|y_1,d_1)p(y_2|\theta,d_2) d\theta} \\
	=& \frac{p(d_1)p(d_2|y_1,d_1)\ p(\theta)p(y_1|\theta, d_1)p(y_2|\theta,d_2)}{p(d_1)p(d_2|y_1,d_1)\ \int p(\theta)p(y_1|\theta, d_1)p(y_2|\theta,d_2) d\theta} \\
	=& \frac{p(\theta)p(y_1|\theta, d_1)p(y_2|\theta,d_2)}{\int p(\theta)p(y_1|\theta, d_1)p(y_2|\theta,d_2) d\theta}
\end{align}
showing that the dependence between $d_2$ and $d_1, y_1$ need not bother us. A related question from \cite{berry1985} was whether the map that takes the observed data to the posterior is a measurable function. This was addressed in \cite[pp. 18-20]{berry1985} in the restricted setting of the multi-armed bandit.

A powerful answer to the second question was given by \cite{paninski2005} and is a form of Bernstein--von Mises Theorem for EIG maximisation OED. Under relatively mild conditions, for any neighbourhood $\mathcal{U}$ of the true parameter value $\theta_0$ we have
\begin{equation}
	p(\mathcal{U} | y_{1:T}, d_{1:T}) \to 1 \text{ as } T \to \infty \text{ in probability}
\end{equation}
Under further conditions, we can show that the posteriors $p(\theta | y_{1:T}, d_{1:T})$ are asymptotically Normal with covariance matrix $\Sigma_\text{info}$. The same result holds for i.i.d. sampling of designs, giving rise to $\Sigma_\text{iid}$. We have that $\text{det}\ \Sigma_\text{info} \le \text{det}\ \Sigma_\text{iid}$. This led \cite{paninski2005} to say ``Thus, information maximization is in a rigorous sense asymptotically more efficient than the i.i.d. sampling strategy.'' Related results were obtained by \cite{pronzato2010} and \cite{hu1998}.

\section{Estimation of EIG}
\label{sec:esteig}
The estimation of EIG, and quantities mathematically equivalent to it, has received attention from a diverse group of researchers.

\subsection{Challenges in EIG estimation}
\label{sec:challenges}
Recall
\begin{align}
	\label{eig:post}
	\eig(d) =
	\iint  p(y, \theta | d) \log \frac{p(\theta | y, d)}{p(\theta)} \, dy\,  d\theta
	=\iint  p(y, \theta | d) \log \frac{p(y | \theta, d)}{p(y|d)} \, dy\,  d\theta.
\end{align}
The computation of this integral is challenging since neither $p(\theta|y,d)$ nor $p(y|d)$ nor the outer integral can, in general, be found in closed form.

A further complication arises when the likelihood $p(y|\theta,d)$ cannot be computed pointwise. For example, this is the case in the presence of nuisance variables, also known as random effects. 
These are additional latent variables, $\psi$, that we do not consider variables of interest and so we do not want to waste resources reducing our uncertainty for them. 
Such models arise frequently in scientific applications, for instance accounting for individual variation between participants in a survey. With random effects $\psi$ we have
\begin{equation}
	p(y|\theta, d) = \int p(y|\theta, \psi, d) p(\psi | \theta) d\psi
\end{equation}
which is typically intractable.

We survey existing approaches to EIG estimation.

\subsection{Nested Monte Carlo}
\label{sec:nmc}
The estimator of \cite{vincent2017} and \cite{myung2013}, among others, is a nested Monte Carlo (NMC) estimator
\begin{equation}
	\eig(d) \approx \frac{1}{N}\sum_{n=1}^N \left[ \log p(y_n | \theta_n, d) - \log \left(\frac{1}{M} \sum_{m=1}^M p(y_n | \theta_m, d) \right) \right]
\end{equation}
where
\begin{align}
	y_n, \theta_n \simiid & \ p(y, \theta | d) \\
	\theta_m \simiid & \ p(\theta)
\end{align}
are all independent.

The drawbacks of such an estimator were noted in \cite{nmc}. Most notably, while simple Monte Carlo estimators converge with a mean squared error rate $\mathcal{O}(N^{-1})$ in the total number of samples, NMC estimators converge at a much slower $\mathcal{O}(N^{-2/3})$ rate and are biased, though consistent \cite{nmc}.

A saving grace of the MC approach is a speed-up due to \cite{vincent2017} in the case that $\mathcal{Y}$, the sample space of $y$, is finite. Then
\begin{equation}
	\eig(d) \approx \sum_y \left[ \frac{1}{N}\sum_{n=1}^N p(y | \theta_n, d) \log p(y | \theta_n, d) -  \left(\frac{1}{N} \sum_{n=1}^N p(y | \theta_n, d) \right) \log \left( \frac{1}{N} \sum_{n=1}^N p(y | \theta_n, d) \right) \right]
\end{equation}
where
\begin{equation}
	\theta_n \simiid \ p(\theta).
\end{equation}
Since this is a continuous function of vanilla Monte Carlo estimators, it converges at a rate $\mathcal{O}(N^{-1})$.

We finally mention that, whilst not present in the literature, NMC can be readily extended to the case of random effects
\begin{equation}
	\eig(d) \approx \frac{1}{N}\sum_{n=1}^N \left[ \log \left(\frac{1}{M'} \sum_{m'=1}^{M'} p(y_n | \theta_n, \psi_{nm'}, d) \right) - \log \left(\frac{1}{M} \sum_{m=1}^M p(y_n | \theta_m, \psi_m, d) \right) \right]
\end{equation}
where
\begin{align}
	y_n, \theta_n \simiid & \ p(y, \theta | d) \\
	\psi_{nm'} | \theta_n \sim & \ p(\psi | \theta_n) \\
	\theta_m, \psi_m \simiid & \ p(\theta, \psi)
\end{align}
and we note that we can replace $\psi_{nm'}$ with $\psi_m$ when $\theta$ and $\psi$ are independent under the prior $p(\theta, \psi)$. Again, when $\mathcal{Y}$ is finite a speed-up is possible, although we can no longer avoid nesting Monte Carlo estimators.

\subsection{Inference based approaches}
A number of authors \cite{long2013, ryan2015} use some form of Laplace approximation to $p(\theta|y,d)$ to estimate expected information gain. In \cite{ouyang2016}, probabilistic programming is used to completely solve the inference problem (in finite spaces) en route to estimating EIG.

\subsection{Mutual information estimation}
\label{sec:mie}
As mentioned previously, EIG estimation is mathematically equivalent to mutual information estimation, a topic that has received recent attention in part due to the connection with Generative Adversarial Networks (GANs) \cite{infogan} and disentanglement \cite{tianqichen}. In the following section, we drop $d$ from our graphical model, and consider a joint density $p(y, \theta) = p(\theta)p(y|\theta)$.

Since $p(\theta|y)$ is typically an intractable, we might use an approximation $q(\theta|y)$. An idea used by \cite{ba} and \cite{infogan} is to bound the mutual information in terms of an amortised posterior $q(\theta|y)$ as
\begin{align}
	\text{MI}(y, \theta) =& \int p(\theta) \int p(y|\theta) \log \frac{p(\theta | y)}{p(\theta)} \, dy \, d\theta \\
	\ge & \int p(\theta) \int p(y|\theta) \log \frac{q(\theta | y)}{p(\theta)} \, dy \, d\theta.
\end{align}
For a fuller derivation and discussion of this and related bounds, see Chapter~\ref{chap:voed} on variational optimal experiment design.

A more recent idea is to use the Donsker-Varadhan Representation of the KL divergence to estimate mutual information \cite{mine}. We have
\begin{equation}
	\label{eq:dv}
	\text{MI}(y, \theta) = \kl{p(y, \theta)}{p(y)p(\theta)} = \sup_T \left\{ \expect_{p(y,\theta)} [T(y, \theta)] - \log\left(\expect_{p(\theta)p(y)}[e^{T(y,\theta)}] \right)  \right\}
\end{equation}
where the supremum is taken over measurable $T$. Note that the optimising $T$ is given by
\begin{equation}
	T^*(y, \theta) = \log\frac{p(y,\theta)}{p(y)p(\theta)} + C \text{ where }C\text{ is any constant}
\end{equation}
The importance of \eqref{eq:dv} is that we no longer need access to any densities to estimate the mutual information.

Practical implementations arising from both these objective functions follow from choosing a suitable parametric family for $T$ (or, in the former case, for $q(\theta|y)$). One then optimises the bound w.r.t. the parameters of the family using finite sample approximations to the expectations. We note that such an idea is intimately connected with the GAN \cite{fgan}.

\section{Optimisation of EIG}
\label{sec:opteig}
So far, little attention has been paid to the design $d$. We suppose now that $d \in \mathcal{D}$ and we seek
\begin{equation}
	d^* = \argmax_{d\in\mathcal{D}} \quad \eig(d)
\end{equation}
the optimal design. As outlined in the previous section, we can have only approximate estimates of $\eig(d)$. Thus we seek a global optimum of a unknown function which we have only noisy evaluations of. This puts us squarely in the domain of Bayesian optimisation \cite{shahriari2016}. When $\mathcal{D}$ is finite, we might instead call it a bandit problem.

Bayesian optimisation in its simplest form requires
\begin{enumerate}
	\item A model of the unknown function
	\item An acquisition rule to decide which design(s) should be queried at the next iteration
\end{enumerate}
It is a fascinating fact that we are now back in the setting of OED. The variable of interest is the location of $d^*$, the maximiser of the unknown function. Other features of the function can be regarded as random effects. See \cite{pes} for further discussion on the connection between Bayesian optimisation and OED, in particular, the connection of EIG to Bayesian optimisation.

A popular approach to Bayesian optimisation is to choose a Gaussian process (GP) model of the unknown function, and an upper confidence bound (UCB) acquisition rule \cite{srinivas2009}.

One aspect that sets $\eig$ maximisation apart from conventional Bayesian optimisation is the ability for us to obtain more accurate estimates of the unknown function $\eig$ by varying the amount of computational resources assigned to estimation. This was explored by \cite{vincent2017} in a finite $\mathcal{D}$ setting. The number of NMC samples was increased for the most promising designs. \cite{mcleod2017} tackled a more general problem of variable cost objectives, taking a GP based approach.

\section{Applications}
Optimal experiment design is a broadly applicable subject and has been re-discovered a number of times in various fields. The purpose of this section is twofold: firstly, to note specific problems tackled by OED and to identify the model $p(y, \theta | d)$ and design space $\mathcal{D}$ in use; secondly, to more broadly suggest which types of models and experiment design problems are relevant to a certain field.

\subsection{Bayesian optimisation}
We begin with a very brief section on Bayesian optimisation, as we discussed this area in \ref{sec:opteig}.

In Bayesian optimisation, models are regression models and typically nonlinear in nature. Gaussian process models are popular. The parameter of interest is the location of the optimum \\ \cite{pes}. The design space $\mathcal{D}$ consists of either single points in the input space $\mathcal{X}$ (single acquisition), or vectors with entries in $\mathcal{X}$ (multiple acquisition). A number of experiment design strategies, here referred to as acquisition rules, have been proposed. See \cite{ginsbourger2008, azimi2012} for a discussion of single and multiple acquisition rules.

\subsection{Active learning}
Whilst active learning is formally as broad as OED, we can identify certain models of particular interest in this field.

Beginning with \cite{cohn1996}, the authors look at models for regression such as the neural network, mixture of Gaussians and locally weighted regression. The entire function (or its governing parameters) was the variable of interest. The design space was simply the input space (single acquisition).

We note in passing that an OED treatment of neural networks and deep regression models requires a credible Bayesian approach to deep learning, something that remains an area of research \cite{gal2016}.

The connection between active learning and Bayesian optimal design was explored in \cite{golovin2010}. Here, the parameter space, termed the hypothesis space $\mathcal{H}$, as well as the test space (design space) and outcome space are all finite. The authors begin with a problem in which the outcome of an experiment is deterministic and the goal is to determine exactly which hypothesis is correct. In the noiseless setting, the sequential design can be encoded as a decision tree and the problem is called the Optimal Decision Tree problem. This problem is known to be NP-hard. The extension to probabilistic modelling comes when the outcome of a test is random. See \cite{nowak2009} for a related example.


\subsection{Psychology}
An important use of OED in psychology is to distinguish between competing theories. Here, the parameter space is a finite set of theories and all other variables in the model are regarded as random effects. For instance, \cite{ouyang2016} apply OED to the 5-4 experiment for category learning \cite{medin1978}. The experiment aimed to distinguish two competing models of category learning: the \textit{exemplar model} and the \textit{prototype model}.

In \cite{vincent2017}, OED for delayed and risky choice (DARC) experiments was studied. In such experiments, we want to model how humans discount future rewards relative to present ones. A single experiment takes the following form: `Would you prefer £$A_1$ at time $t_1$ or £$A_2$ at time $t_2$'? The parameter of interest is the discount factor and the model was a (non-linear) variant on probit regression.

Models employed in psychology (and other social sciences) may not be Bayesian. Those doing Bayesian data analysis typically use linear models and their extensions -- generalized linear mixed models (GLMMs) \cite{kruschkebda, gelmanbda}. In many cases, the parameters of interest will be certain coefficients in these models.

\subsection{Big data analysis}
In most applications, we assume that experiments will be carried out after design. In \cite{drovandi2017}, the authors instead assume that a large dataset already exists. The design therefore consists of points, or sets of points, in the existing dataset. The examples focused on logistic regression models. In general, such settings have a well-defined design space, the models to be fitted are regression models.

\subsection{Natural science}
Natural sciences typically have complex and powerful predictive models. In \cite{vanlier2012}, a complex biochemical network was modelled with a differential equation. Highly non-linear equations for amplitude versus offset experiments are just one example from physics \cite{berg2003}. On the other hand, with these well-developed theories the statistical models used in natural sciences can be very simple -- little more than Gaussian measurement error was incorporated in both these examples. The design spaces were the times at which to measure a response function, and the physical set-up of the experiment respectively.


