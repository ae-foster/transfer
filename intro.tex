\section{TODOs}

\begin{itemize}
	\item Probabilistic modelling is important.
	\item Applied stats models
	\item Bayesian nonparametric models
	\item Probabilistic programming helps to do automation
\end{itemize}


\section{Optimal experiment design}
Consider the following graphical model
\begin{center}
\begin{tikzpicture}
	\tikzstyle{every node}=[draw,shape=circle]
	\node[draw,fill=lightgray] (y) at (0, 0) {$y$};
	\node (t) at (2, 1) {$\theta$};
	\node[draw,shape=rectangle] (d) at (-2, 1) {$d$};
	\draw [->] (d) -- (y);
	\draw [->] (t) -- (y);
\end{tikzpicture}
\end{center}
in which $d$ represents a (non-random) control variate, the design of the experiment, $\theta$ represents a latent variable and $y$ represents the observed outcome of the experiment.



1. introduce the problem
2. discuss EIG. Where does EIG come from? Interpretations of EIG
3. literature review-- who uses OED, how have they approached the problem?
4. mutual information lit review


\subsection{Literature review}
TODO: Major revisions needed

\subsubsection{Machine learning and statistics}
There is long-standing interest in `classical' statistical models and their design \cite{youssefreview}. Consider a basic linear models with Gaussian noise. Optimal design here can be expressed in terms of the eigenspectrum of $XX^T$ (see \cite{chaloner1984}). For nonlinear models, see the section on Physics. What about GLMs? Likely can solve the problem analytically again. These are great baselines. People in the linear models case are often concerned with proving the equivalence of different kinds of optimality \cite{youssefreview}.

In machine learning, experiment design is closely related to two common techniques in, for example, image classification: data augmentation and active learning.

In data augmentation images are rotated, translated, etc to create more training data. We could theoretically optimise the augmentation but this seems wasteful since copying the labels to new images is very easy.

A much more interesting area is \textit{active learning}. In this context, there are a large number of unlabeled images. Labeling is expensive. We select which images to label either up front, or (more typical in active learning) in a sequential manner. The key difference here is that we have a finite pool of unlabeled instances. We may be more interested in reducing uncertainty in the labels of these unlabeled images than in our posterior entropy.

The connection between active learning and Bayesian optimal design was explored in \cite{golovin2010}. In this paper, they start from a place where the outcome of a test is deterministic (think of the 12 men on an island problem). In the noiseless setting, the sequential design can be encoded as a decision tree and the problem is called the Optimal Decision Tree problem. This problem is known to be NP-hard. The OED criterion is introduced later to account for noisy observations and the fact that true parameters need not be known exactly even after all tests have been run.

A particular active learning example can be found in \cite{nowak2009}. We have $\mathcal{H}$ a hypothesis space (read parameter space) and $\mathcal{X}$ a query space (read design space). The goal is to determine the true $h^* \in \mathcal{H}$. Each query outputs a label in $\{-1, 1\}$ corrupted with Bernoulli noise (independent between queries). The algorithm broadly works by targeting $x \in \mathcal{X}$ where the expected posterior label in near $0$ (random guess). The convergence rate of $\prob(\hat{h}_i \ne h^*) \to 0$ is studied (shown exponential). The importance of having access to unlabeled data is exploited by \cite{dasgupta2006}.

\subsubsection{Psychology}
For an overview of optimal experiment design in probabilistic programming, \cite{ouyang2016} from Noah's group is a good place to start. Experiment design is necessary to distinguish competing theories. We should select models with the highest \textit{expected information gain}, written formally as 
\begin{equation}
U(d) = \expect_{(Y, \Theta) \sim p(y, \theta | d)}\left\{\log \frac{p(\Theta | Y, d)}{p(\Theta)}\right\}
\end{equation}
This equation has been studied by mathematical statisticians since the 50's \cite{lindley1956}. We can naively evaluate $U(d)$ in a PPL via \textit{nested inference}.

A canonical experiment discussed in this paper is the 5-4 experiment for category learning \cite{medin1978}. The experiment aimed to distinguish two competing models of category learning: the \textit{exemplar model} (learn categories by comparing new items to all previous items) and the \textit{prototype model} (learn categories by remembering a prototypical example). There are two models, so $\Theta = \{m_1, m_2\}$. During the experiment, participants are presented with a sequence of objects. In the training phase, they are also told the correct label after guessing. In testing they have no feedback. The objects varied in four dimensions: colour, shape, size and count; we can consider the space of objects to be $\{0,1\}^4$. Each object has a label $A$ or $B$. The true labeling mechanism was limited by Medin and Schaffer to be linearly separable. There are 9 inputs in the final testing set and Medin and Schaffer restricted there to be 5 $A$s and 4 $B$s. The objects $0000$ and $1111$ have to be present. Under these restrictions there are 933 possible experiments up to permutation. So $\mathcal{D}$ is a finite set of size 933. $\mathcal{Y}$ is the `test' responses, ie. the subjects responses when they are not given feedback. Thus $\mathcal{Y} = \{A, B\}^9$. The kind of participant numbers seen were 10-30.

In \cite{vincent2017}, the canonical experiment is as follows. We want to model how humans discount future rewards relative to present ones (via utility indifference pricing framework). A single experiment takes the following form: `Would you prefer £$A_1$ at time $t_1$ or £$A_2$ at time $t_2$'? The parameter of interest is the discount factor. Formally, $\mathcal{D} = [0, \infty)^2$, $\mathcal{Y} = \{1, 2\}$ and $\Theta = [0, \infty)$. These three spaces fit together as follows. We first chose $\mathcal{D}$ the space of possible designs. We subsequently chose $\mathcal{Y}$ the space of possible outcomes. We posited a probabilistic model for $Y$ in terms of parameters $\theta$. Focus on non-nested estimation for finite $\mathcal{Y}$ and sequential design. Sequential design means different participants will be asked different questions based on their previous answers.

\subsubsection{Bioinformatics}

In \cite{vanlier2012}, the authors consider experiment design from the perspective that, with little data, many different parameter settings adequately describe the data. Canonical model. Biochemical network modeled as an ODE.
\begin{align*}
&\dot{x} = f(x, u, p) \\
&\dot{y} = g(x, q) + \xi \\
&x(0) = x_0
\end{align*}
$u$ is the input, $x, y$ are time varying (uncontrolled) with $x$ latent and $y$ observed, $p, q, x_0$ are parameters $\theta$ required to simulate the model and do not depend on $t$, $\xi$ represents measurement noise. We treat $\xi$ as iid Gaussian. The paradigm chosen here is expected variance reduction, as opposed to information gain. (Possibly wrong if people still do that.) The variance is in the posterior predictive density.

\subsubsection{Physics}
In \cite{berg2003}, we begin by discussing `classical' experiment design procedures which assume linear dependence between model and outcome $y = G_{m_0}d$. One can solve this linear equation by least squares, $\hat{d} = G^T(GG^T)^{-1}y$. Define $L = G^T(GG^T)^{-1}$, possibly adding regularization as necessary. Basically, you want to maximize the max eigenvalue of $G$, which is essentially a gradient. The larger gradient, the more informative the experiment. In a linear setting, the gradient does not depend on the true parameter value.

Now consider linear noise but a nonlinear function between parameters and outcomes. For example, the authors took
\begin{equation}
R_p = \left(\frac{1}{2}[1 + \tan^{-1} i] - 4c^2 \sin^2 i\right) \frac{\Delta \alpha}{\alpha}
\end{equation}
where $\alpha = (\alpha 1 + \alpha2)/2$, $\Delta \alpha = (\alpha 2 - \alpha 1)$. The parameter we want to optimize is $\alpha2$.

This is a relatively simple and comprehensible case.