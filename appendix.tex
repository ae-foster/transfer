\section{Variational families for GLMMs}
\label{sec:glmms}
The success of variational OED relies heavily on a good choice of variational family (guide, in the language of Pyro). Here we briefly outline techniques that aid writing guides for Generalised Linear Mixed Models (GLMMs), which we define hierarchically as
\begin{align}
	\theta, \psi &\sim N(\mu, \Sigma) \\
	\eta &= X_{d, \theta} \theta + X_{d, \psi} \psi \\
	y &\sim p(y|\eta) \text{ with } \expect[y|\eta] = g^{-1}(\eta).
\end{align}

We first consider posterior guides, $\qp$, and propose a multivariate Gaussian approximation. It can be shown that the bound \eqref{eq:postbound} is maximised when the mean and covariance of $\qp$ match that of the true posterior. We have
\begin{align}
	\expect(\theta|y, d) &= \expect(\expect[\theta|\eta, d]|y, d) \\
	\label{eq:eeta}
	\expect[\theta|\eta, d] &= G_d \eta \\
	\implies \expect(\theta|y, d) &= G_d \expect(\eta|y, d).
\end{align}
Similarly, the covariance $\cov(\theta|y,d)$ is
\begin{align}
	\cov(\theta | y, d) &= \expect(\cov[\theta | \eta, d] | y,d) + \cov( \expect[\theta | \eta, d] | y, d) \\
	\cov[\theta | \eta, d] &= H_d \\
	\cov( \expect[\theta | \eta, y, d] | y, d) &= 
	G_d \cov(\eta|y, d) G_d^T \text{ from \eqref{eq:eeta}} \\
	\implies \cov(\theta | y, d) &= H_d + G_d \cov(\eta|y, d) G_d^T
\end{align}
The matrices $G_d$ and $H_d$ can be computed explicitly
\begin{align}
	H_d^{-1} &= \Sigma_{\theta\theta}^{-1} + X_{d, \theta}^T(X_{d, \psi}[\Sigma_{\psi\psi} - \Sigma_{\psi\theta}\Sigma_{\theta\theta}^{-1}\Sigma_{\theta\psi}]X_{d, \psi}^T)^{-1}X_{d, \theta}, \\
	G_d &= H_d(\Sigma_{\theta\theta}^{-1}\mu_\theta + X_{d, \theta}^T(X_{d, \psi}[\Sigma_{\psi\psi} - \Sigma_{\psi\theta}\Sigma_{\theta\theta}^{-1}\Sigma_{\theta\psi}]X_{d, \psi}^T)^{-1}(\eta - X_{d, \psi}\mu_\psi)).
\end{align}
In some cases, $\eta | y, d$ may be significantly easier to learn than $\theta | y, d$.

Turning now to marginal guides, $\qm$, and likelihood guides, $\ql$, we first note that the likelihood $p(y|\eta)$ is critical in determining the correct guide. A response $y$ in $\{0, 1\}$ needs a very different guide to one in $\R$. Any suitable density estimator may be used for $\qm(y|d)$. For $\ql(y|\theta,d)$ we note that $y$ depends on $\theta$ only through $X_{d, \theta}\theta$. In many cases, simply shifting certain parameters by $X_{d, \theta}\theta$ can give a good approximation. For logistic regression, for example, we wish to estimate
\begin{equation}
	\int p(\epsilon) f(X_{d,\theta}\theta + \epsilon) d\epsilon = f(X_{d,\theta}\theta) + \tfrac{1}{2} \text{Var}(\epsilon)f''(X_{d,\theta}\theta) + ...
\end{equation}
where $\epsilon = X_{d,\psi}\psi$ assuming for now $\mu_\psi=0$, and $f$ is the logistic function. We approximate this as
\begin{equation}
	\tfrac{1}{2}\left(f(X_{d,\theta}\theta + \phi) + f(X_{d,\theta}\theta - \phi)\right).
\end{equation}



\section{Experiment details}
\label{sec:expdeets}

\subsection{LinReg}
A classical Bayesian linear regression model has the following form
\begin{align}
	\theta &\sim N(\mu_\theta, \Sigma_{\theta\theta}) \\
	y | \theta, d & \sim N(X_d\theta, \sigma^2 I)
\end{align}
where $X_d$ is the design matrix.

In our LinReg example, we took:
\begin{align}
	\mu_\theta &= 0 \\
	\Sigma_{\theta\theta} &= \begin{pmatrix}
		10^2 & 0 \\
		0 & 0.1^2
	\end{pmatrix} \\
	\sigma^2 &= 1 \\
	X_d &=
	\begin{pmatrix}
		1 & 0 \\
		\vdots & \vdots \\
		1 & 0 \\
		0 & 1 \\
		\vdots & \vdots \\
		0 & 1
	\end{pmatrix} \text{ a } (10 \times 2) \text{ matrix}
\end{align}
with all 11 possible designs considered.

We chose families of variational distributions that include the true posterior (or true marginal). For the amortised posterior, we set $\phi = (\Lambda, \delta, \Sigma_\text{p})$ and let
\begin{align}
	\qp(\theta | y, d; \phi) &\sim N(\mu_\text{p}, \Sigma_\text{p}) \\
	\text{where } \mu_\text{p} &= (X_d^T X_d+\Lambda)^{-1}X_d^T(y + \delta)
\end{align}
and $\Lambda$ is a diagonal matrix and $\Sigma_\text{p}$ is positive definite. For the marginal, we simple take $\phi = (\mu_\text{m}, \Sigma_\text{m})$ and
\begin{equation}
	\qm(y|d;\phi) \sim N(\mu_\text{m}, \Sigma_\text{m})
\end{equation}

Finally, for each of our variational methods we used the Adam optimizer \cite{kingma2014adam} with a learning rate specified below. Each iteration used  $N_t$ samples, with $T$ iterations in total. We used $N$ samples for the final evaluation. NMC settings are $N, M$ \cite{vincent2017} and we took the advice of the authors to set $N = M^2$.

The exact parameter settings, to get about 10 seconds of computation for each method, were
\begin{center}
\begin{tabu}{|c|c|c|c|c|c|c|c|c|c|}
\hline
	\multicolumn{2}{|c|}{NMC} & \multicolumn{4}{|c|}{Posterior} & \multicolumn{4}{|c|}{Marginal} \\
	\hline
	$N$ & $M$ & $N_t$ & $T$ & lr & $N$ & $N_t$ & $T$ & lr & $N$ \\
	\hline
	$110^2$ & 110 & 10 & 1200 & 0.05 & 500 & 10 & 1200 & 0.05 & 500 \\
	\hline
\end{tabu}
\end{center}

\subsection{LinReg + RE}
In this experiment, we extended the model to include random effects. Specifically, 
\begin{align}
	\theta &\sim N(\mu_\theta, \Sigma_{\theta\theta}) \\
	\psi &\sim N(\mu_\psi, \Sigma_{\psi\psi}) \\
	y | \theta, d & \sim N(X_{d,\theta}\theta+ X_{d,\psi}\psi, \sigma^2 I)
\end{align}
where
\begin{align}
	\mu_\psi &= 0 \\
	\Sigma_{\psi\psi} &= I_{10} \\
	X_{d,\psi} &= I_{10}
\end{align}
and $X_{d,\theta}$ was the $X_d$ from the previous example. Here $\theta$ is the random variable of interest, while $\psi$ is a nuisance variable that needs to be integrated out.
The variational distribution for the likelihood, $\ql$, was the same as $\qm$, except that the mean was shifted by $X_{d,\theta}\theta$.

The exact parameter settings, to get about 10 seconds of computation for each method, were
\begin{center}
\begin{tabu}{|c|c|c|c|c|c|c|c|c|c|}
\hline
	\multicolumn{2}{|c|}{NMC} & \multicolumn{4}{|c|}{Posterior} & \multicolumn{4}{|c|}{Marginal} \\
	\hline
	$N$ & $M$ & $N_t$ & $T$ & lr & $N$ & $N_t$ & $T$ & lr & $N$ \\
	\hline
	$52^2$ & 52 & 10 & 150 & 0.05 & 500 & 10 & 600 & 0.05 & 500 \\
	\hline
\end{tabu}
\end{center}

\subsection{LinReg large ${\rm dim}(y)$}
This experiment was identical to LinReg, except that we took $X_d$ to have dimensions $20 \times 2$, with 11 designs as before. We also altered the marginal variational distribution to reflect the new dimension of $y$. Other than that, the specification of all variational distributions was identical.

The exact parameter settings, to get about 10 seconds of computation for each method, were
\begin{center}
\begin{tabu}{|c|c|c|c|c|c|c|c|c|c|}
\hline
	\multicolumn{2}{|c|}{NMC} & \multicolumn{4}{|c|}{Posterior} & \multicolumn{4}{|c|}{Marginal} \\
	\hline
	$N$ & $M$ & $N_t$ & $T$ & lr & $N$ & $N_t$ & $T$ & lr & $N$ \\
	\hline
	$90^2$ & 90 & 10 & 1000 & 0.05 & 500 & 10 & 700 & 0.05 & 500 \\
	\hline
\end{tabu}
\end{center}

\subsection{N$\Gamma^{-1}$Reg}
We changed the model to 
\begin{align}
	\sigma^2 &\sim \Gamma^{-1}(\alpha, \beta) \\
	\theta &\sim N(\mu_\theta, \Sigma_{\theta\theta}) \\
	y | \theta, \sigma^2, d & \sim N(X_d\theta, \sigma^2 I)
\end{align}
where $\alpha=3$ and $\beta=2$.

We used a mean-field posterior variational distribution. For $\theta$, we used the same variational distribution as for LinReg. 
For $\sigma^2$ we used an inverse Gamma variational distribution. We augmented the parameters $\phi$ with $\alpha_\text{p}, b_0$ and took $\beta_\text{p} = b_0 + \tfrac{1}{2}(y^Ty - y^TX_d\mu_\text{p})$. Then
\begin{equation}
	\qp(\sigma^2 | y, d; \phi) \sim \Gamma^{-1}(\alpha_\text{p}, \beta_\text{p})
\end{equation}

The marginal variational distribution was as in LinReg (a Gaussian).

The exact parameter settings, to get about 10 seconds of computation for each method, were
\begin{center}
\begin{tabu}{|c|c|c|c|c|c|c|c|c|c|}
\hline
	\multicolumn{2}{|c|}{NMC} & \multicolumn{4}{|c|}{Posterior} & \multicolumn{4}{|c|}{Marginal} \\
	\hline
	$N$ & $M$ & $N_t$ & $T$ & lr & $N$ & $N_t$ & $T$ & lr & $N$ \\
	\hline
	$110^2$ & 110 & 10 & 800 & 0.05 & 500 & 10 & 1200 & 0.05 & 500 \\
	\hline
\end{tabu}
\end{center}

\subsection{LinReg2}
We used the model of LinReg, and the following parameter settings
\begin{center}
\begin{tabu}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
	\multicolumn{4}{|c|}{Posterior} & \multicolumn{4}{|c|}{Posterior, with analytic entropy} & \multicolumn{4}{|c|}{Donsker-Varadhan} \\
	\hline
	$N_t$ & $T$ & lr & $N$ & $N_t$ & $T$ & lr & $N$ & $N_t$ & $T$ & lr & $N$ \\
	\hline
	10 & 1200 & 0.05 & 500 & 10 & 1200 & 0.05 & 50 & 100 & 100 & 0.05 & 500 \\
	\hline
\end{tabu}
\end{center}


%\subsection{SigReg}
%We first considered a linear regression model, but with different parameters
%\begin{align}
%	\mu_0 &= (1 \quad 5) \\
%	\Sigma_0 &= \begin{pmatrix}
%		0.25^2 & 0 \\
%		0 & 4^2
%	\end{pmatrix} \\
%	\sigma^2 &= 1 \\
%	X_d &=
%	\begin{pmatrix}
%		x & 1
%	\end{pmatrix} \text{ a } (1 \times 2) \text{ matrix and } x \in [-15, 15] 
%\end{align}
%We also altered the model, adding a sigmoid transformation
%\begin{equation}
%	y'|\theta, d = \text{sigmoid}(k \cdot y)
%\end{equation}
%where $y$ is as in LinReg, and $k=2$.