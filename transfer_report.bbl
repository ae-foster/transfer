\begin{thebibliography}{}

\bibitem[Azimi et~al., 2012]{azimi2012}
Azimi, J., Jalali, A., and Fern, X. (2012).
\newblock Hybrid batch bayesian optimization.
\newblock {\em arXiv preprint arXiv:1202.5597}.

\bibitem[Barber and Agakov, 2004]{ba}
Barber, D. and Agakov, F. (2004).
\newblock The im algorithm: a variational approach to information maximization.
\newblock {\em Advances in Neural Information Processing Systems}, 16:201.

\bibitem[Belghazi et~al., 2018]{mine}
Belghazi, I., Rajeswar, S., Baratin, A., Hjelm, R.~D., and Courville, A.
  (2018).
\newblock Mine: mutual information neural estimation.
\newblock {\em arXiv preprint arXiv:1801.04062}.

\bibitem[Berry and Fristedt, 1985]{berry1985}
Berry, D.~A. and Fristedt, B. (1985).
\newblock Bandit problems: sequential allocation of experiments (monographs on
  statistics and applied probability).
\newblock {\em London: Chapman and Hall}, 5.

\bibitem[Bingham et~al., 2018]{pyro}
Bingham, E., Chen, J.~P., Jankowiak, M., Obermeyer, F., Pradhan, N.,
  Karaletsos, T., Singh, R., Szerlip, P., Horsfall, P., and Goodman, N.~D.
  (2018).
\newblock {Pyro: Deep Universal Probabilistic Programming}.
\newblock {\em arXiv preprint arXiv:1810.09538}.

\bibitem[Bloem-Reddy et~al., 2018]{bntl}
Bloem-Reddy, B., Foster, A., Mathieu, E., and Teh, Y.~W. (2018).
\newblock Sampling and inference for beta neutral-to-the-left models of sparse
  networks.
\newblock In {\em Uncertainty in Artifical Intelligence}.

\bibitem[Bloem-Reddy et~al., 2017]{bnpppl}
Bloem-Reddy, B., Mathieu, E., Foster, A., Rainforth, T., Teh, Y.~W., Ge, H.,
  Lomel{\'\i}, M., and Ghahramani, Z. (2017).
\newblock Sampling and inference for discrete random probability measures in
  probabilistic programs.
\newblock In {\em NIPS Workshop on Advances in Approximate Bayesian Inference}.

\bibitem[Box, 1976]{box1976}
Box, G.~E. (1976).
\newblock Science and statistics.
\newblock {\em Journal of the American Statistical Association},
  71(356):791--799.

\bibitem[Box, 1982]{box1982}
Box, G.~E. (1982).
\newblock Choice of response surface design and alphabetic optimality.
\newblock Technical report, WISCONSIN UNIV-MADISON MATHEMATICS RESEARCH CENTER.

\bibitem[Burda et~al., 2015]{iwae}
Burda, Y., Grosse, R., and Salakhutdinov, R. (2015).
\newblock Importance weighted autoencoders.
\newblock {\em arXiv preprint arXiv:1509.00519}.

\bibitem[Chaloner and Verdinelli, 1995]{chaloner1995}
Chaloner, K. and Verdinelli, I. (1995).
\newblock Bayesian experimental design: A review.
\newblock {\em Statistical Science}, pages 273--304.

\bibitem[Chen et~al., 2018]{tianqichen}
Chen, T.~Q., Li, X., Grosse, R., and Duvenaud, D. (2018).
\newblock Isolating sources of disentanglement in variational autoencoders.
\newblock {\em arXiv preprint arXiv:1802.04942}.

\bibitem[Chen et~al., 2016]{infogan}
Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P.
  (2016).
\newblock Infogan: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2172--2180.

\bibitem[Chernoff, 1972]{chernoff1972sequential}
Chernoff, H. (1972).
\newblock {\em Sequential analysis and optimal design}, volume~8.
\newblock Siam.

\bibitem[Cohn et~al., 1996]{cohn1996}
Cohn, D.~A., Ghahramani, Z., and Jordan, M.~I. (1996).
\newblock Active learning with statistical models.
\newblock {\em Journal of artificial intelligence research}, 4:129--145.

\bibitem[Dayan et~al., 1995]{dayan1995helmholtz}
Dayan, P., Hinton, G.~E., Neal, R.~M., and Zemel, R.~S. (1995).
\newblock The helmholtz machine.
\newblock {\em Neural computation}, 7(5):889--904.

\bibitem[de~Heide and Gr{\"u}nwald, 2017]{de2017}
de~Heide, R. and Gr{\"u}nwald, P.~D. (2017).
\newblock Why optional stopping is a problem for bayesians.
\newblock {\em arXiv preprint arXiv:1708.08278}.

\bibitem[Drovandi et~al., 2017]{drovandi2017}
Drovandi, C.~C., Holmes, C., McGree, J.~M., Mengersen, K., Richardson, S., and
  Ryan, E.~G. (2017).
\newblock Principles of experimental design for big data analysis.
\newblock {\em Statistical science: a review journal of the Institute of
  Mathematical Statistics}, 32(3):385.

\bibitem[Fedorov, 1972]{fedorov1972}
Fedorov, V. (1972).
\newblock {\em Theory of optimal experiments}.
\newblock Academic Press, New York.

\bibitem[Gal and Ghahramani, 2016]{gal2016}
Gal, Y. and Ghahramani, Z. (2016).
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em international conference on machine learning}, pages
  1050--1059.

\bibitem[Gelman et~al., 2013]{gelmanbda}
Gelman, A., Stern, H.~S., Carlin, J.~B., Dunson, D.~B., Vehtari, A., and Rubin,
  D.~B. (2013).
\newblock {\em Bayesian data analysis}.
\newblock Chapman and Hall/CRC.

\bibitem[Ginsbourger et~al., 2008]{ginsbourger2008}
Ginsbourger, D., Le~Riche, R., and Carraro, L. (2008).
\newblock A multi-points criterion for deterministic parallel global
  optimization based on gaussian processes.

\bibitem[Golovin et~al., 2010]{golovin2010}
Golovin, D., Krause, A., and Ray, D. (2010).
\newblock Near-optimal bayesian active learning with noisy observations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  766--774.

\bibitem[Gonz{\'a}lez et~al., 2016]{gonzalez2016}
Gonz{\'a}lez, J., Osborne, M., and Lawrence, N. (2016).
\newblock Glasses: Relieving the myopia of bayesian optimisation.
\newblock In {\em Artificial Intelligence and Statistics}, pages 790--799.

\bibitem[Goodfellow et~al., 2014]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y. (2014).
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680.

\bibitem[Hern{\'a}ndez-Lobato et~al., 2014]{pes}
Hern{\'a}ndez-Lobato, J.~M., Hoffman, M.~W., and Ghahramani, Z. (2014).
\newblock Predictive entropy search for efficient global optimization of
  black-box functions.
\newblock In {\em Advances in neural information processing systems}, pages
  918--926.

\bibitem[Hu, 1998]{hu1998}
Hu, I. (1998).
\newblock On sequential designs in nonlinear problems.
\newblock {\em Biometrika}, 85(2):496--503.

\bibitem[Kingma and Ba, 2014]{kingma2014adam}
Kingma, D.~P. and Ba, J. (2014).
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}.

\bibitem[Kingma and Welling, 2014]{kingma2014auto}
Kingma, D.~P. and Welling, M. (2014).
\newblock Auto-encoding variational {Bayes}.
\newblock In {\em ICLR}.

\bibitem[Kruschke, 2014]{kruschkebda}
Kruschke, J. (2014).
\newblock {\em Doing Bayesian data analysis: A tutorial with R, JAGS, and
  Stan}.
\newblock Academic Press.

\bibitem[Lindley, 1956]{lindley1956}
Lindley, D.~V. (1956).
\newblock On a measure of the information provided by an experiment.
\newblock {\em The Annals of Mathematical Statistics}, pages 986--1005.

\bibitem[Lindley, 1972]{lindley1972}
Lindley, D.~V. (1972).
\newblock {\em Bayesian statistics, a review}, volume~2.
\newblock SIAM.

\bibitem[Long et~al., 2013]{long2013}
Long, Q., Scavino, M., Tempone, R., and Wang, S. (2013).
\newblock Fast estimation of expected information gains for bayesian
  experimental designs based on laplace approximations.
\newblock {\em Computer Methods in Applied Mechanics and Engineering},
  259:24--39.

\bibitem[Lueckmann et~al., 2018]{lueckmann2018}
Lueckmann, J.-M., Bassetto, G., Karaletsos, T., and Macke, J.~H. (2018).
\newblock Likelihood-free inference with emulator networks.
\newblock {\em arXiv preprint arXiv:1805.09294}.

\bibitem[Marchant et~al., 2014]{marchant2014}
Marchant, R., Ramos, F., Sanner, S., et~al. (2014).
\newblock Sequential bayesian optimisation for spatial-temporal monitoring.
\newblock In {\em UAI}, pages 553--562.

\bibitem[McLeod et~al., 2017]{mcleod2017}
McLeod, M., Osborne, M.~A., and Roberts, S.~J. (2017).
\newblock Practical bayesian optimization for variable cost objectives.
\newblock {\em arXiv preprint arXiv:1703.04335}.

\bibitem[Medin and Schaffer, 1978]{medin1978}
Medin, D.~L. and Schaffer, M.~M. (1978).
\newblock Context theory of classification learning.
\newblock {\em Psychological review}, 85(3):207.

\bibitem[Myung et~al., 2013]{myung2013}
Myung, J.~I., Cavagnaro, D.~R., and Pitt, M.~A. (2013).
\newblock A tutorial on adaptive design optimization.
\newblock {\em Journal of mathematical psychology}, 57(3-4):53--67.

\bibitem[Nowak, 2009]{nowak2009}
Nowak, R. (2009).
\newblock Noisy generalized binary search.
\newblock In {\em Advances in neural information processing systems}, pages
  1366--1374.

\bibitem[Nowozin et~al., 2016]{fgan}
Nowozin, S., Cseke, B., and Tomioka, R. (2016).
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  271--279.

\bibitem[Osborne et~al., 2009]{osborne2009gaussian}
Osborne, M.~A., Garnett, R., and Roberts, S.~J. (2009).
\newblock Gaussian processes for global optimization.
\newblock In {\em 3rd international conference on learning and intelligent
  optimization (LION3)}, pages 1--15.

\bibitem[Ouyang et~al., 2016]{ouyang2016}
Ouyang, L., Tessler, M.~H., Ly, D., and Goodman, N. (2016).
\newblock Practical optimal experiment design with probabilistic programs.
\newblock {\em arXiv preprint arXiv:1608.05046}.

\bibitem[Paige and Wood, 2016]{paige2016inference}
Paige, B. and Wood, F. (2016).
\newblock Inference networks for sequential monte carlo in graphical models.
\newblock In {\em International Conference on Machine Learning}, pages
  3040--3049.

\bibitem[Pang et~al., 2018]{pang2018}
Pang, K., Dong, M., Wu, Y., and Hospedales, T. (2018).
\newblock Meta-learning transferable active learning policies by deep
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1806.04798}.

\bibitem[Paninski, 2005]{paninski2005}
Paninski, L. (2005).
\newblock Asymptotic theory of information-theoretic experimental design.
\newblock {\em Neural Computation}, 17(7):1480--1507.

\bibitem[Pearl et~al., 2009]{pearl2009causal}
Pearl, J. et~al. (2009).
\newblock Causal inference in statistics: An overview.
\newblock {\em Statistics surveys}, 3:96--146.

\bibitem[Piera and Parada, 2009]{piera2009}
Piera, F.~J. and Parada, P. (2009).
\newblock On convergence properties of shannon entropy.
\newblock {\em Problems of Information Transmission}, 45(2):75--94.

\bibitem[Pronzato, 2010]{pronzato2010}
Pronzato, L. (2010).
\newblock One-step ahead adaptive d-optimal design on a finite design space is
  asymptotically optimal.
\newblock {\em Metrika}, 71(2):219--238.

\bibitem[Rainforth et~al., 2018]{nmc}
Rainforth, T., Cornish, R., Yang, H., Warrington, A., and Wood, F. (2018).
\newblock On nesting monte carlo estimators.
\newblock In {\em International Conference on Machine Learning}, pages
  4264--4273.

\bibitem[Rezende and Mohamed, 2015]{rezende2015variational}
Rezende, D.~J. and Mohamed, S. (2015).
\newblock Variational inference with normalizing flows.
\newblock {\em arXiv preprint arXiv:1505.05770}.

\bibitem[Rezende et~al., 2014]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D. (2014).
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em ICML}.

\bibitem[Robbins and Monro, 1951]{robbins1951stochastic}
Robbins, H. and Monro, S. (1951).
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407.

\bibitem[Rouder, 2014]{rouder2014}
Rouder, J.~N. (2014).
\newblock Optional stopping: No problem for bayesians.
\newblock {\em Psychonomic Bulletin \& Review}, 21(2):301--308.

\bibitem[Ryan et~al., 2018]{ryanreview}
Ryan, E., Drovandi, C., McGree, J., and Pettitt, A. (2018).
\newblock Fully bayesian optimal experimental design: A review.
\newblock {\em Preprint eprints.qut.edu.au/75000/1/75000.pdf}.

\bibitem[Ryan et~al., 2015]{ryan2015}
Ryan, E.~G., Drovandi, C.~C., and Pettitt, A.~N. (2015).
\newblock Fully bayesian experimental design for pharmacokinetic studies.
\newblock {\em Entropy}, 17(3):1063--1089.

\bibitem[Shafer et~al., 2011]{shafer2011}
Shafer, G., Shen, A., Vereshchagin, N., Vovk, V., et~al. (2011).
\newblock Test martingales, bayes factors and p-values.
\newblock {\em Statistical Science}, 26(1):84--101.

\bibitem[Shahriari et~al., 2016]{shahriari2016}
Shahriari, B., Swersky, K., Wang, Z., Adams, R.~P., and De~Freitas, N. (2016).
\newblock Taking the human out of the loop: A review of bayesian optimization.
\newblock {\em Proceedings of the IEEE}, 104(1):148--175.

\bibitem[Shepard, 1962]{shepard1962analysis}
Shepard, R.~N. (1962).
\newblock The analysis of proximities: multidimensional scaling with an unknown
  distance function. i.
\newblock {\em Psychometrika}, 27(2):125--140.

\bibitem[Silver and Veness, 2010]{pomcp}
Silver, D. and Veness, J. (2010).
\newblock Monte-carlo planning in large pomdps.
\newblock In {\em Advances in neural information processing systems}, pages
  2164--2172.

\bibitem[Srinivas et~al., 2009]{srinivas2009}
Srinivas, N., Krause, A., Kakade, S.~M., and Seeger, M. (2009).
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock {\em arXiv preprint arXiv:0912.3995}.

\bibitem[Stuhlm{\"u}ller et~al., 2013]{stuhlmuller2013learning}
Stuhlm{\"u}ller, A., Taylor, J., and Goodman, N. (2013).
\newblock Learning stochastic inverses.
\newblock In {\em Advances in neural information processing systems}, pages
  3048--3056.

\bibitem[Tabak and Turner, 2013]{tabak2013family}
Tabak, E. and Turner, C.~V. (2013).
\newblock A family of nonparametric density estimation algorithms.
\newblock {\em Communications on Pure and Applied Mathematics}, 66(2):145--164.

\bibitem[Torgerson, 1952]{torgerson1952multidimensional}
Torgerson, W.~S. (1952).
\newblock Multidimensional scaling: I. theory and method.
\newblock {\em Psychometrika}, 17(4):401--419.

\bibitem[Tucker et~al., 2017]{tucker2017rebar}
Tucker, G., Mnih, A., Maddison, C.~J., Lawson, J., and Sohl-Dickstein, J.
  (2017).
\newblock Rebar: Low-variance, unbiased gradient estimates for discrete latent
  variable models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2627--2636.

\bibitem[van Den~Berg et~al., 2003]{berg2003}
van Den~Berg, J., Curtis, A., and Trampert, J. (2003).
\newblock Optimal nonlinear bayesian experimental design: an application to
  amplitude versus offset experiments.
\newblock {\em Geophysical Journal International}, 155(2):411--421.

\bibitem[Vanlier et~al., 2012]{vanlier2012}
Vanlier, J., Tiemann, C.~A., Hilbers, P.~A., and van Riel, N.~A. (2012).
\newblock A bayesian approach to targeted experiment design.
\newblock {\em Bioinformatics}, 28(8):1136--1142.

\bibitem[Vincent and Rainforth, 2017]{vincent2017}
Vincent, B.~T. and Rainforth, T. (2017).
\newblock The darc toolbox: automated, flexible, and efficient delayed and
  risky choice experiments using bayesian adaptive design.

\end{thebibliography}
