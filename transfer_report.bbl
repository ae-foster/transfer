\begin{thebibliography}{}

\bibitem[Azimi et~al., 2012]{azimi2012}
Azimi, J., Jalali, A., and Fern, X. (2012).
\newblock Hybrid batch bayesian optimization.
\newblock {\em arXiv preprint arXiv:1202.5597}.

\bibitem[Barber and Agakov, 2004]{ba}
Barber, D. and Agakov, F. (2004).
\newblock The im algorithm: a variational approach to information maximization.
\newblock {\em Advances in Neural Information Processing Systems}, 16:201.

\bibitem[Belghazi et~al., 2018]{mine}
Belghazi, I., Rajeswar, S., Baratin, A., Hjelm, R.~D., and Courville, A.
  (2018).
\newblock Mine: mutual information neural estimation.
\newblock {\em arXiv preprint arXiv:1801.04062}.

\bibitem[Berry and Fristedt, 1985]{berry1985}
Berry, D.~A. and Fristedt, B. (1985).
\newblock Bandit problems: sequential allocation of experiments (monographs on
  statistics and applied probability).
\newblock {\em London: Chapman and Hall}, 5.

\bibitem[Bloem-Reddy et~al., 2018]{bntl}
Bloem-Reddy, B., Foster, A., Mathieu, E., and Teh, Y.~W. (2018).
\newblock Sampling and inference for beta neutral-to-the-left models of sparse
  networks.
\newblock In {\em Uncertainty in Artifical Intelligence}.

\bibitem[Bloem-Reddy et~al., 2017]{bnpppl}
Bloem-Reddy, B., Mathieu, E., Foster, A., Rainforth, T., Teh, Y.~W., Ge, H.,
  Lomel{\'\i}, M., and Ghahramani, Z. (2017).
\newblock Sampling and inference for discrete random probability measures in
  probabilistic programs.
\newblock In {\em NIPS Workshop on Advances in Approximate Bayesian Inference}.

\bibitem[Box, 1982]{box1982}
Box, G.~E. (1982).
\newblock Choice of response surface design and alphabetic optimality.
\newblock Technical report, WISCONSIN UNIV-MADISON MATHEMATICS RESEARCH CENTER.

\bibitem[Chaloner and Verdinelli, 1995]{chaloner1995}
Chaloner, K. and Verdinelli, I. (1995).
\newblock Bayesian experimental design: A review.
\newblock {\em Statistical Science}, pages 273--304.

\bibitem[Chen et~al., 2018]{tianqichen}
Chen, T.~Q., Li, X., Grosse, R., and Duvenaud, D. (2018).
\newblock Isolating sources of disentanglement in variational autoencoders.
\newblock {\em arXiv preprint arXiv:1802.04942}.

\bibitem[Chen et~al., 2016]{infogan}
Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P.
  (2016).
\newblock Infogan: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2172--2180.

\bibitem[Cohn et~al., 1996]{cohn1996}
Cohn, D.~A., Ghahramani, Z., and Jordan, M.~I. (1996).
\newblock Active learning with statistical models.
\newblock {\em Journal of artificial intelligence research}, 4:129--145.

\bibitem[Drovandi et~al., 2017]{drovandi2017}
Drovandi, C.~C., Holmes, C., McGree, J.~M., Mengersen, K., Richardson, S., and
  Ryan, E.~G. (2017).
\newblock Principles of experimental design for big data analysis.
\newblock {\em Statistical science: a review journal of the Institute of
  Mathematical Statistics}, 32(3):385.

\bibitem[Fedorov, 1972]{fedorov1972}
Fedorov, V. (1972).
\newblock {\em Theory of optimal experiments}.
\newblock Academic Press, New York.

\bibitem[Gal and Ghahramani, 2016]{gal2016}
Gal, Y. and Ghahramani, Z. (2016).
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em international conference on machine learning}, pages
  1050--1059.

\bibitem[Gelman et~al., 2013]{gelmanbda}
Gelman, A., Stern, H.~S., Carlin, J.~B., Dunson, D.~B., Vehtari, A., and Rubin,
  D.~B. (2013).
\newblock {\em Bayesian data analysis}.
\newblock Chapman and Hall/CRC.

\bibitem[Ginsbourger et~al., 2008]{ginsbourger2008}
Ginsbourger, D., Le~Riche, R., and Carraro, L. (2008).
\newblock A multi-points criterion for deterministic parallel global
  optimization based on gaussian processes.

\bibitem[Golovin et~al., 2010]{golovin2010}
Golovin, D., Krause, A., and Ray, D. (2010).
\newblock Near-optimal bayesian active learning with noisy observations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  766--774.

\bibitem[Gonz{\'a}lez et~al., 2016]{gonzalez2016}
Gonz{\'a}lez, J., Osborne, M., and Lawrence, N. (2016).
\newblock Glasses: Relieving the myopia of bayesian optimisation.
\newblock In {\em Artificial Intelligence and Statistics}, pages 790--799.

\bibitem[Hern{\'a}ndez-Lobato et~al., 2014]{pes}
Hern{\'a}ndez-Lobato, J.~M., Hoffman, M.~W., and Ghahramani, Z. (2014).
\newblock Predictive entropy search for efficient global optimization of
  black-box functions.
\newblock In {\em Advances in neural information processing systems}, pages
  918--926.

\bibitem[Hu, 1998]{hu1998}
Hu, I. (1998).
\newblock On sequential designs in nonlinear problems.
\newblock {\em Biometrika}, 85(2):496--503.

\bibitem[Kruschke, 2014]{kruschkebda}
Kruschke, J. (2014).
\newblock {\em Doing Bayesian data analysis: A tutorial with R, JAGS, and
  Stan}.
\newblock Academic Press.

\bibitem[Lindley, 1956]{lindley1956}
Lindley, D.~V. (1956).
\newblock On a measure of the information provided by an experiment.
\newblock {\em The Annals of Mathematical Statistics}, pages 986--1005.

\bibitem[Lindley, 1972]{lindley1972}
Lindley, D.~V. (1972).
\newblock {\em Bayesian statistics, a review}, volume~2.
\newblock SIAM.

\bibitem[Long et~al., 2013]{long2013}
Long, Q., Scavino, M., Tempone, R., and Wang, S. (2013).
\newblock Fast estimation of expected information gains for bayesian
  experimental designs based on laplace approximations.
\newblock {\em Computer Methods in Applied Mechanics and Engineering},
  259:24--39.

\bibitem[Marchant et~al., 2014]{marchant2014}
Marchant, R., Ramos, F., Sanner, S., et~al. (2014).
\newblock Sequential bayesian optimisation for spatial-temporal monitoring.
\newblock In {\em UAI}, pages 553--562.

\bibitem[McLeod et~al., 2017]{mcleod2017}
McLeod, M., Osborne, M.~A., and Roberts, S.~J. (2017).
\newblock Practical bayesian optimization for variable cost objectives.
\newblock {\em arXiv preprint arXiv:1703.04335}.

\bibitem[Medin and Schaffer, 1978]{medin1978}
Medin, D.~L. and Schaffer, M.~M. (1978).
\newblock Context theory of classification learning.
\newblock {\em Psychological review}, 85(3):207.

\bibitem[Myung et~al., 2013]{myung2013}
Myung, J.~I., Cavagnaro, D.~R., and Pitt, M.~A. (2013).
\newblock A tutorial on adaptive design optimization.
\newblock {\em Journal of mathematical psychology}, 57(3-4):53--67.

\bibitem[Nowak, 2009]{nowak2009}
Nowak, R. (2009).
\newblock Noisy generalized binary search.
\newblock In {\em Advances in neural information processing systems}, pages
  1366--1374.

\bibitem[Nowozin et~al., 2016]{fgan}
Nowozin, S., Cseke, B., and Tomioka, R. (2016).
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  271--279.

\bibitem[Ouyang et~al., 2016]{ouyang2016}
Ouyang, L., Tessler, M.~H., Ly, D., and Goodman, N. (2016).
\newblock Practical optimal experiment design with probabilistic programs.
\newblock {\em arXiv preprint arXiv:1608.05046}.

\bibitem[Pang et~al., 2018]{pang2018}
Pang, K., Dong, M., Wu, Y., and Hospedales, T. (2018).
\newblock Meta-learning transferable active learning policies by deep
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1806.04798}.

\bibitem[Paninski, 2005]{paninski2005}
Paninski, L. (2005).
\newblock Asymptotic theory of information-theoretic experimental design.
\newblock {\em Neural Computation}, 17(7):1480--1507.

\bibitem[Pronzato, 2010]{pronzato2010}
Pronzato, L. (2010).
\newblock One-step ahead adaptive d-optimal design on a finite design space is
  asymptotically optimal.
\newblock {\em Metrika}, 71(2):219--238.

\bibitem[Rainforth et~al., 2018]{nmc}
Rainforth, T., Cornish, R., Yang, H., Warrington, A., and Wood, F. (2018).
\newblock On nesting monte carlo estimators.
\newblock In {\em International Conference on Machine Learning}, pages
  4264--4273.

\bibitem[Ryan et~al., 2015]{ryan2015}
Ryan, E.~G., Drovandi, C.~C., and Pettitt, A.~N. (2015).
\newblock Fully bayesian experimental design for pharmacokinetic studies.
\newblock {\em Entropy}, 17(3):1063--1089.

\bibitem[Shahriari et~al., 2016]{shahriari2016}
Shahriari, B., Swersky, K., Wang, Z., Adams, R.~P., and De~Freitas, N. (2016).
\newblock Taking the human out of the loop: A review of bayesian optimization.
\newblock {\em Proceedings of the IEEE}, 104(1):148--175.

\bibitem[Srinivas et~al., 2009]{srinivas2009}
Srinivas, N., Krause, A., Kakade, S.~M., and Seeger, M. (2009).
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock {\em arXiv preprint arXiv:0912.3995}.

\bibitem[van Den~Berg et~al., 2003]{berg2003}
van Den~Berg, J., Curtis, A., and Trampert, J. (2003).
\newblock Optimal nonlinear bayesian experimental design: an application to
  amplitude versus offset experiments.
\newblock {\em Geophysical Journal International}, 155(2):411--421.

\bibitem[Vanlier et~al., 2012]{vanlier2012}
Vanlier, J., Tiemann, C.~A., Hilbers, P.~A., and van Riel, N.~A. (2012).
\newblock A bayesian approach to targeted experiment design.
\newblock {\em Bioinformatics}, 28(8):1136--1142.

\bibitem[Vincent and Rainforth, 2017]{vincent2017}
Vincent, B.~T. and Rainforth, T. (2017).
\newblock The darc toolbox: automated, flexible, and efficient delayed and
  risky choice experiments using bayesian adaptive design.

\end{thebibliography}
